{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ3FsuIkOT-d"
      },
      "source": [
        "# **Fashion MNIST**\n",
        "\n",
        "In this practical application notebook, we will work with fashion MNIST dataset to carry out a classification exercise using Artificial Neural Networks.\n",
        "-----------------------\n",
        "## **Dataset**\n",
        "---------------------------\n",
        "The dataset, Fashion MNIST, is a collection of apparel images falling into several classes. \n",
        "Classes are numbered from 0 to 9 and have the following meanings with Tshirt/Top represented as 0 and an Ankle Boot as 9. \n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "-------------------------\n",
        "## **Objective**\n",
        "--------------------------\n",
        "In this exercise, we will create a simple ANN model to classify the images into some categories\n",
        "\n",
        "-----------------------\n",
        "## **Toolkit**\n",
        "--------------------------\n",
        "We will use TensforFlow, tensorflow implementation of keras on google colab for this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvUohxCpOT-f"
      },
      "source": [
        "## **Loading the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9baBpVWEm1zG"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvoXI41LOT-g"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrAysf29OT-l"
      },
      "outputs": [],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pibEX_Y7OT-o"
      },
      "source": [
        "### **Loading the Data**\n",
        "\n",
        "Let's import the data from the tf.keras.datasets and prepare the train and the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud9LISdBOT-s"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "(X_train, trainY), (X_test,testY) = tf.keras.datasets.fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXlFuNxkOT-v"
      },
      "outputs": [],
      "source": [
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cYXeMfL4KFY"
      },
      "outputs": [],
      "source": [
        "X_train.shape[1] * X_train.shape[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODg1GKNjrvyl"
      },
      "source": [
        "- This suggests that there are 60000 images of size 28\\*28 in the training set and 10000 images of size 28*28 in the test set.\n",
        "- Note that we will need to flatten these images before fitting an ANN model. \n",
        "- Let us now explore the classes present in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTv--93OzgK7"
      },
      "outputs": [],
      "source": [
        "np.unique(trainY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WOs6a_qshhb"
      },
      "source": [
        "- This suggests that the train set has 10 classes where each class denotes one type of apparel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAsqvxY3OT-5"
      },
      "source": [
        "### **Encoding the target variable**\n",
        "\n",
        "- We need to one hot encode the target variable to be able to form the training target vector.\n",
        "- Hint: check tf.keras.utils.to_categorical() - https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnkT3fKNOT-7"
      },
      "outputs": [],
      "source": [
        "y_train = tf.keras.utils.to_categorical(trainY,num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(testY,num_classes=10)\n",
        "\n",
        "# Let's have a look at the shapes of all the datasets\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlBrvB9u2D0w"
      },
      "outputs": [],
      "source": [
        "## Let's normalize the dataset. Since there are pixel values ranging from 0-255, let us divide by 255 to get the new ranges from 0-1\n",
        "X_train = X_train/255\n",
        "X_test = X_test/255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDUVBMnjucoG"
      },
      "source": [
        "### **Visualization**\n",
        "- Now, let us visualize the data items.\n",
        "- We will visualize the first 24 images in the training dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZpTSevIucB2"
      },
      "outputs": [],
      "source": [
        "class_names_list = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "for i in range(24):\n",
        "    plt.subplot(4,6,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(X_train[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names_list[trainY[i]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A5KWzG4OT_F"
      },
      "source": [
        "### **Model Building**\n",
        "\n",
        "- We will now start with the model building process.\n",
        "- We will create a model with\n",
        " - A layer to flatten the input\n",
        " - A hidden layer with 64 nodes (You can play around with this number) and 'relu' activation.\n",
        " - Output layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YomFZLpam1zM"
      },
      "source": [
        "### **Model-1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFhz7B63g195"
      },
      "source": [
        "### **Question 1: Add the output layer with activation function and number of neurons required based on the problem statement.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxCb8meKOT_F"
      },
      "outputs": [],
      "source": [
        "# Initialize sequential model\n",
        "\n",
        "model_1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    ____________________________________________   # Remove this and complete the code.\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfoXMomt0M97"
      },
      "outputs": [],
      "source": [
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1QGGLIuUY5f"
      },
      "source": [
        "**Observations**\n",
        "- The summary of the model shows each layer's name, type, output shape, and the number of parameters at that particular layer.\n",
        "- It also shows the total number of trainable and non-trainable parameters in the model. A parameter whose value is learned while training the model is called a trainable parameter otherwise it is called a non-trainable parameter.\n",
        "- The Flatten layer simply flattens each image into a size of 784 (28*28) and there is no learning or training at this layer. Hence, the number of parameters is 0 for the Flatten layer.\n",
        "- Each image in the form of 784 nodes would be the input for the 'dense' layer. Each node of the previous layer would be connected with each node of the current layer. Also, each connection has one weight to learn and each node has one bias. So, the total number of parameters are (784*64)+64 = 50,240.\n",
        "- Similarly, the last layer - 'dense_1' have (64*10)+10 = 650 parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kenJlE8w1L3v"
      },
      "source": [
        "Let us now compile the model.\n",
        "- We will use 'adam' optimization and 'CategoricalCrossentropy' Loss as the loss. We will track the accuracy metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bECxIrhr1LMo"
      },
      "outputs": [],
      "source": [
        "model_1.compile(optimizer='adam', loss='categorical_crossentropy',  metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSLIehy418ak"
      },
      "outputs": [],
      "source": [
        "# Let us now fit the model\n",
        "\n",
        "fit_history = model_1.fit(X_train, y_train,validation_split=0.1, verbose=1, epochs=10, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A78132QTYGT"
      },
      "source": [
        "**Observation**\n",
        "- We can observe that the model's accuracy increases with the increase in the number of epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4f4mUq184Ei"
      },
      "source": [
        "### **Evaluate the model on the test set**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k0q_nJ5Z4Xr"
      },
      "source": [
        "- Let's predict using the test data. The .predict() method in Keras models returns the probabilities of each observation belonging to each class. We will choose the class where the predicted probability is the highest.\n",
        "- Also, let's build a function to print the classification report and confusion matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoDAt_KBelpd"
      },
      "outputs": [],
      "source": [
        "def metrics_score(actual, predicted):\n",
        "    from sklearn.metrics import classification_report\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    print(classification_report(actual, predicted))\n",
        "    cm = confusion_matrix(actual, predicted)\n",
        "    plt.figure(figsize=(8,5))\n",
        "    sns.heatmap(cm, annot=True,  fmt='.0f', xticklabels=class_names_list, yticklabels=class_names_list)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5M4VYgQm1zN"
      },
      "source": [
        "### **Question 2: What is the test accuracy for the model1?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEc72H6NOT_I"
      },
      "outputs": [],
      "source": [
        "model_1.evaluate(______, _________, verbose = 1)\n",
        "test_pred1 = np.argmax(model_1.predict(________), axis = -1)\n",
        "test_pred1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv-zKPR1m1zO"
      },
      "source": [
        "### **Question 3: Which category has been most correctly classified by the model1?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCeUDbdLdbvj"
      },
      "outputs": [],
      "source": [
        "metrics_score(testY, test_pred1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ScUVXE1X5Fq"
      },
      "source": [
        "**Observations**\n",
        "- Class 6 (Shirt) has the lowest recall and precision. The model is not able to identify the shirt. The confusion matrix shows that the model has predicted shirts as T-shirts/top, Pullover, and Coat which is understandable as these items have similar looks. \n",
        "- Let's try changing the learning rate and train the model for more epochs and see if the model can identify even subtle differences in different objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSx4VI8iOT_m"
      },
      "source": [
        "### **Further Iterations to model building**\n",
        "- Let's change the learning rate and epochs and observe the effect on accuracy on the earlier network.\n",
        "- Let's build a bigger network with the new learning rate and epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUFLLFbqm1zO"
      },
      "source": [
        "### **Model-2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5pDUsQdOT_m"
      },
      "outputs": [],
      "source": [
        "# Initialize sequential model\n",
        "\n",
        "model_2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC8xsO66BsKh"
      },
      "outputs": [],
      "source": [
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRd0VSXUgGMA"
      },
      "source": [
        "**Observation**\n",
        "- The summary remains the same as the previous model because we have not changed anything about the structure of the NN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLVSnCE0OT_1"
      },
      "outputs": [],
      "source": [
        "fit_history_2 = model_2.fit(X_train, y_train, epochs=30, validation_split=0.1, batch_size=64, verbose = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so0SNgLQhHkg"
      },
      "source": [
        "**Observations**\n",
        "- We can see that the accuracy of the training data has increased by ~3% but the accuracy on the validation set has increased only by ~0.50% as compared to the model trained with 10 epochs.\n",
        "- This indicates that if we further increase the number of epochs while keeping everything else the same then the model might start to overfit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbuSU9pym1zP"
      },
      "outputs": [],
      "source": [
        "model_2.evaluate(_________,________, verbose = 1)\n",
        "\n",
        "test_pred2 = np.argmax(model_2.predict(_________), axis  = -1)\n",
        "\n",
        "metrics_score(testY, test_pred2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eabHULjZm1zP"
      },
      "source": [
        "### **Model-3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq2dwSeql9gE"
      },
      "source": [
        "### **Question 4: For the above model i.e Model2, add 1 hidden layer with 128 neurons and relu activation function after the flatten layer. The test accuracy of this model lies between,**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vttvV4m4B0aR"
      },
      "outputs": [],
      "source": [
        "# Initialize sequential model\n",
        "\n",
        "model_3 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    ____________________________________________,  # Remove this and complete the code.\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs8Yie6GEjPa"
      },
      "outputs": [],
      "source": [
        "model_3.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44vpKE9WjqIj"
      },
      "source": [
        "**Observations**\n",
        "- We can see that the number of parameters has increased by ~2.15 times than the number of parameters in previous models.\n",
        "- Increasing the number of parameters can significantly increase the training time of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NAq_0iSm1zP"
      },
      "outputs": [],
      "source": [
        "model_3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "fit_history_3 = model_3.fit(X_train, y_train, epochs=30, validation_split=0.1, batch_size=64, verbose = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrqQ7XGSiimv"
      },
      "source": [
        "**Observations**\n",
        "- The validation accuracy of the model has further increased by ~0.71% and the training accuracy has further increased by ~1.4%. So, there is still a hint of overfitting.\n",
        "- We can play around with hyperparameters of the model or try different layer structures to improve the model performance and reduce the overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_ZRCUE5kZ_Z"
      },
      "source": [
        "- We can see that accuracy keeps increasing for the test data as the number of epochs increased but validation accuracy has become somewhat constant after 10 epochs.\n",
        "- This indicates that the model learns the training data more closely after each epoch but cannot replicate the performance on the validation data which is a sign of overfitting.\n",
        "- The same pattern can be observed for loss as well. It keeps decreasing for the training data with the increase in epochs but becomes somewhat constant for the validation data after 10 epochs.\n",
        "\n",
        "Now, let's make final predictions on the test data using the last model we built."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGrKu8lnsDHM"
      },
      "source": [
        "### **Final Predictions on the Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILByJ7kbLsH1"
      },
      "outputs": [],
      "source": [
        "final_pred = np.argmax(model_3.predict(X_test), axis  = -1)\n",
        "\n",
        "metrics_score(testY, final_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bXtXI6FpA5p"
      },
      "source": [
        "- The precision and recall for class 6 (Shirt) have increased. The confusion matrix shows that the model is still not able to differentiate between T-shirt/top and Shirt but became better in differentiating Shirt with Pullover and Coat.\n",
        "- The model has become even better at identifying Trouser. It has an f1-score of 98% for class 1 (Trouser).\n",
        "- The overall accuracy on the test data is approximately the same as the validation accuracy.\n",
        "\n",
        "### **Let's visualize the images from the test data.** \n",
        "- We will randomly select 24 images from the test data and visualize them.\n",
        "- The title of each image would show the actual and predicted label of that image and the probability of the predicted class.\n",
        "- Higher the probability more confident the model is about the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dYY0lO_rqY1"
      },
      "outputs": [],
      "source": [
        "rows = 4\n",
        "cols = 6\n",
        "fig = plt.figure(figsize=(15, 15))\n",
        "for i in range(cols):\n",
        "    for j in range(rows):\n",
        "        random_index = np.random.randint(0, len(testY))\n",
        "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
        "        ax.imshow(X_test[random_index, :])\n",
        "        pred_label = class_names_list[final_pred[random_index]]\n",
        "        true_label = class_names_list[testY[random_index]]\n",
        "        y_pred_test_max_probas = np.max(model_3.predict(X_test), axis=1)\n",
        "        pred_proba = y_pred_test_max_probas[random_index]\n",
        "        ax.set_title(\"actual: {}\\npredicted: {}\\nprobability: {:.3}\\n\".format(\n",
        "               true_label, pred_label, pred_proba\n",
        "        ))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spo1tDLIDU58"
      },
      "source": [
        "### **Comments** \n",
        "\n",
        "- We have trained 3 different models with some changes.\n",
        "- The plots track the variation in the accuracies and the loss across epochs and allow us to map how better these models generalize.\n",
        "- We have observed good performance on the train set but there is some amount of overfitting in the models that get more prominent as we increase the epochs.\n",
        "- We went ahead with model 3 and evaluated the test data on it.\n",
        "- Finally, we visualized some of the images from the test data.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Hands-on quiz ANN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}